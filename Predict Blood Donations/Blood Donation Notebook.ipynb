{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "3"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 1. Inspecting transfusion.data file\n",
    "<p><img src=\"https://assets.datacamp.com/production/project_646/img/blood_donation.png\" style=\"float: right;\" alt=\"A pictogram of a blood bag with blood donation written in it\" width=\"200\"></p>\n",
    "<p>Blood transfusion saves lives - from replacing lost blood during major surgery or a serious injury to treating various illnesses and blood disorders. Ensuring that there's enough blood in supply whenever needed is a serious challenge for the health professionals. According to <a href=\"https://www.webmd.com/a-to-z-guides/blood-transfusion-what-to-know#1\">WebMD</a>, \"about 5 million Americans need a blood transfusion every year\".</p>\n",
    "<p>Our dataset is from a mobile blood donation vehicle in Taiwan. The Blood Transfusion Service Center drives to different universities and collects blood as part of a blood drive. We want to predict whether or not a donor will give blood the next time the vehicle comes to campus.</p>\n",
    "<p>The data is stored in <code>datasets/transfusion.data</code> and it is structured according to RFMTC marketing model (a variation of RFM). We'll explore what that means later in this notebook. First, let's inspect the data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little <a href=\"https://www.optimove.com/resources/learning-center/rfm-segmentation\">explanation</a> about the RFM model =) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "dc": {
     "key": "3"
    },
    "scrolled": true,
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recency (months),Frequency (times),Monetary (c.c. blood),Time (months),\"whether he/she donated blood in March 2007\"\n",
      "\n",
      "2 ,50,12500,98 ,1\n",
      "\n",
      "0 ,13,3250,28 ,1\n",
      "\n",
      "1 ,16,4000,35 ,1\n",
      "\n",
      "2 ,20,5000,45 ,1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/transfusion.data', 'r') as f:\n",
    "    for i in range(5):\n",
    "        line = f.readline()\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "10"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 2. Loading the blood donations data\n",
    "<p>We now know that we are working with a typical CSV file (i.e., the delimiter is <code>,</code>, etc.). We proceed to loading the data into memory.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dc": {
     "key": "10"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)  \\\n",
       "0                 2                 50                  12500             98   \n",
       "1                 0                 13                   3250             28   \n",
       "2                 1                 16                   4000             35   \n",
       "3                 2                 20                   5000             45   \n",
       "4                 1                 24                   6000             77   \n",
       "\n",
       "   whether he/she donated blood in March 2007  \n",
       "0                                           1  \n",
       "1                                           1  \n",
       "2                                           1  \n",
       "3                                           1  \n",
       "4                                           0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transfusion = pd.read_csv(\"datasets/transfusion.data\")\n",
    "\n",
    "transfusion.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "17"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 3. Inspecting transfusion DataFrame\n",
    "<p>Let's briefly return to our discussion of RFM model. RFM stands for Recency, Frequency and Monetary Value and it is commonly used in marketing for identifying your best customers. In our case, our customers are blood donors.</p>\n",
    "<p>RFMTC is a variation of the RFM model. Below is a description of what each column means in our dataset:</p>\n",
    "<ul>\n",
    "<li>R (Recency - months since the last donation)</li>\n",
    "<li>F (Frequency - total number of donation)</li>\n",
    "<li>M (Monetary - total blood donated in c.c.)</li>\n",
    "<li>T (Time - months since the first donation)</li>\n",
    "<li>a binary variable representing whether he/she donated blood in March 2007 (1 stands for donating blood; 0 stands for not donating blood)</li>\n",
    "</ul>\n",
    "<p>It looks like every column in our DataFrame has the numeric type, which is exactly what we want when building a machine learning model. Let's verify our hypothesis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dc": {
     "key": "17"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 748 entries, 0 to 747\n",
      "Data columns (total 5 columns):\n",
      " #   Column                                      Non-Null Count  Dtype\n",
      "---  ------                                      --------------  -----\n",
      " 0   Recency (months)                            748 non-null    int64\n",
      " 1   Frequency (times)                           748 non-null    int64\n",
      " 2   Monetary (c.c. blood)                       748 non-null    int64\n",
      " 3   Time (months)                               748 non-null    int64\n",
      " 4   whether he/she donated blood in March 2007  748 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 29.3 KB\n"
     ]
    }
   ],
   "source": [
    "transfusion.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "24"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 4. Creating target column\n",
    "<p>We are aiming to predict the value in <code>whether he/she donated blood in March 2007</code> column. Let's rename this it to <code>target</code> so that it's more convenient to work with.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "dc": {
     "key": "24"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)  \\\n",
       "0                 2                 50                  12500             98   \n",
       "1                 0                 13                   3250             28   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfusion.rename(\n",
    "    columns={'whether he/she donated blood in March 2007': 'target'},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "transfusion.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "31"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 5. Checking target incidence\n",
    "<p>We want to predict whether or not the same donor will give blood the next time the vehicle comes to campus. The model for this is a binary classifier, meaning that there are only 2 possible outcomes:</p>\n",
    "<ul>\n",
    "<li><code>0</code> - the donor will not give blood</li>\n",
    "<li><code>1</code> - the donor will give blood</li>\n",
    "</ul>\n",
    "<p>Target incidence is defined as the number of cases of each individual target value in a dataset. That is, how many 0s in the target column compared to how many 1s? Target incidence gives us an idea of how balanced (or imbalanced) is our dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dc": {
     "key": "31"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.762\n",
       "1    0.238\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfusion.target.value_counts(normalize=True).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "31"
    }
   },
   "source": [
    "Although I understand that a more balanced dataset would typically provide better training for my model, I've decided to use this particular dataset for a few reasons. Firstly, this dataset is already publicly available and has been used in previous studies, which will allow me to compare my results with others and potentially build on previous research. Additionally, this dataset is specifically focused on blood donation, which is the area I'm interested in exploring for my project. Finally, I believe that I can still make use of techniques such as evaluation metrics that take into account the class imbalance, as well as adjusting the class weights during model training, to mitigate the potential bias towards the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "31"
    }
   },
   "source": [
    "Using stratification would be a good way to address the class imbalance in your dataset. Stratification involves dividing the dataset into several subsets based on the values of a certain feature (in this case, the target variable indicating whether a donor will donate blood again). By stratifying the dataset, you ensure that each subset contains a proportional number of instances from each class. In your case, you can use a stratification ratio of 0.76:0.24 to ensure that each subset contains approximately 76% instances of the negative class and 24% instances of the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "38"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 6. Splitting transfusion into train and test datasets\n",
    "<p>We'll now use <code>train_test_split()</code> method to split <code>transfusion</code> DataFrame.</p>\n",
    "<p>Target incidence informed us that in our dataset <code>0</code>s appear 76% of the time. We want to keep the same structure in train and test datasets, i.e., both datasets must have 0 target incidence of 76%. This is very easy to do using the <code>train_test_split()</code> method from the <code>scikit learn</code> library - all we need to do is specify the <code>stratify</code> parameter. In our case, we'll stratify on the <code>target</code> column.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dc": {
     "key": "38"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1750</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)\n",
       "334                16                  2                    500             16\n",
       "99                  5                  7                   1750             26"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    transfusion.drop(columns='target'),\n",
    "    transfusion.target,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=transfusion.target\n",
    ")\n",
    "\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "45"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 7. Selecting model using TPOT\n",
    "<p><a href=\"https://github.com/EpistasisLab/tpot\">TPOT</a> is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.</p>\n",
    "<p><img src=\"https://assets.datacamp.com/production/project_646/img/tpot-ml-pipeline.png\" alt=\"TPOT Machine Learning Pipeline\"></p>\n",
    "<p>TPOT will automatically explore hundreds of possible pipelines to find the best one for our dataset. Note, the outcome of this search will be a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">scikit-learn pipeline</a>, meaning it will include any pre-processing steps as well as the model.</p>\n",
    "<p>We are using TPOT to help us zero in on one model that we can then explore and optimize further.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "dc": {
     "key": "45"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.7422459184429089\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.7422459184429089\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.7422459184429089\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.7422459184429089\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.7456308339276876\n",
      "\n",
      "Best pipeline: MultinomialNB(Normalizer(input_matrix, norm=l2), alpha=0.001, fit_prior=True)\n",
      "\n",
      "AUC score: 0.7637\n",
      "\n",
      "Best pipeline steps:\n",
      "1. Normalizer()\n",
      "2. MultinomialNB(alpha=0.001)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "tpot = TPOTClassifier(\n",
    "    generations=5,\n",
    "    population_size=20,\n",
    "    verbosity=2,\n",
    "    scoring='roc_auc',\n",
    "    random_state=42,\n",
    "    disable_update_check=True,\n",
    "    config_dict='TPOT light'\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "tpot_auc_score = roc_auc_score(y_test, tpot.predict_proba(X_test)[:, 1])\n",
    "print(f'\\nAUC score: {tpot_auc_score:.4f}')\n",
    "\n",
    "print('\\nBest pipeline steps:', end='\\n')\n",
    "for idx, (name, transform) in enumerate(tpot.fitted_pipeline_.steps, start=1):\n",
    "    # Print idx and transform\n",
    "    print(f'{idx}. {transform}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "45"
    }
   },
   "source": [
    "### TPOT Analysis\n",
    "        \n",
    "TPOT picked LogisticRegression as the best model for our dataset with no pre-processing steps, giving us the AUC score of 0.7850. This is a great starting point. Let's see if we can make it better. To make it easier, I will explain a little bit about the AUC and Logistic Regression\n",
    "\n",
    "The AUC (Area Under the Curve) is a performance metric commonly used in binary classification problems to evaluate the performance of a machine learning model. It measures the ability of the model to distinguish between the positive and negative classes (i.e., the donors who will donate blood again and those who will not). The AUC score ranges from 0 to 1, with higher values indicating better performance. A score of 0.5 indicates that the model is no better than random guessing, while a score of 1 indicates perfect classification. For more information about AUC and it's metrics <a href=\"https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl\">Click here</a>.\n",
    "\n",
    "\n",
    "In our case, TPOT has selected Logistic Regression as the best model for our dataset, and has given you an AUC score of 0.7850. This means that the model is able to distinguish between donors who will donate blood again and those who will not with a high degree of accuracy. However, there is always room for improvement, and you can try various techniques such as feature engineering, hyperparameter tuning, and different model architectures to further improve the AUC score and make the model even more accurate.\n",
    "<p><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*klFuUpBGVAjTfpTak2HhUA.png\" alt=\"Logistic Regression Exemple\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "52"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 8. Checking the variance\n",
    "<p>TPOT picked <code>LogisticRegression</code> as the best model for our dataset with no pre-processing steps, giving us the AUC score of 0.7850. This is a great starting point. Let's see if we can make it better.</p>\n",
    "<p>One of the assumptions for linear models is that the data and the features we are giving it are related in a linear fashion, or can be measured with a linear distance metric. If a feature in our dataset has a high variance that's orders of magnitude greater than the other features, this could impact the model's ability to learn from other features in the dataset.</p>\n",
    "<p>Correcting for high variance is called normalization. It is one of the possible transformations you do before training a model. Let's check the variance to see if such transformation is needed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "dc": {
     "key": "52"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recency (months)              66.929\n",
       "Frequency (times)             33.830\n",
       "Monetary (c.c. blood)    2114363.700\n",
       "Time (months)                611.147\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.var().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "52"
    }
   },
   "source": [
    "Normalization is a crucial step in preparing our data for machine learning. It involves scaling the values of our features so that they fall within a certain range, usually between 0 and 1, or -1 and 1. This can help to address issues with high variance in our data, where some features may have much larger values than others. \n",
    "\n",
    "By normalizing our data, we can ensure that all features are treated equally by our model and that no single feature dominates the others. This can improve the accuracy and generalization performance of our model, especially for algorithms that rely on distance metrics, such as k-nearest neighbors or support vector machines.\n",
    "\n",
    "There are several ways to normalize data for use in machine learning models. Some of the most common techniques include:\n",
    "\n",
    "*Min-Max Scaling*: this technique scales the values of each feature to a specific range, usually between 0 and 1. This is done by subtracting the minimum value of the feature and dividing by the maximum range.\n",
    "\n",
    "*Z-score normalization*: this technique normalizes the values of each feature to have mean zero and standard deviation 1. This is done by subtracting the feature's mean and dividing by the standard deviation.\n",
    "\n",
    "*Unit Vector Scaling*: this technique normalizes the values of each resource to have norm 1, which means that the sum of squares of the values equals 1.\n",
    "\n",
    "*Robust Scaler*: this technique scales the values of each feature by taking the median and quartiles instead of the mean and standard deviation. This makes it less sensitive to outliers.\n",
    "\n",
    "*Log Transformation*: This technique applies a logarithmic transformation to the feature values, which can help reduce variation in extreme values.\n",
    "\n",
    "These are just some of the normalization techniques commonly used in machine learning. Choosing the appropriate technique depends on the dataset and model being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "59"
    }
   },
   "source": [
    "### Log transformation - Explanation\n",
    "\n",
    "Log transformation is a technique commonly used to normalize data in machine learning. It involves taking the logarithm of the values of a feature, which can help to reduce the impact of extreme values and make the distribution of the data more symmetrical. In particular, the log transformation is often used for data that is skewed to the right, meaning that the majority of the values are low, but there are some very high values that can throw off the analysis.\n",
    "\n",
    "When performing a log transformation, the values are scaled by taking the natural logarithm (ln) or the base-10 logarithm of the original values. The resulting transformed values will be smaller for larger original values, which can help to bring extreme values closer to the center of the distribution. This can be particularly useful for features that exhibit a power-law relationship, such as income or population size, where the relationship between the variables is not linear but rather grows exponentially.\n",
    "\n",
    "Overall, log transformation is a useful tool for preprocessing data in machine learning, particularly when dealing with skewed distributions or power-law relationships. However, it's important to note that log transformation can't be applied to zero or negative values, so you may need to add a small constant to the values before taking the logarithm.\n",
    "<p><img src=\"https://www.medcalc.org/manual/images/logtransformation.png\" alt=\"Log Transformation Exemple\"> width=200 </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "59"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 9. Log normalization\n",
    "<p><code>Monetary (c.c. blood)</code>'s variance is very high in comparison to any other column in the dataset. This means that, unless accounted for, this feature may get more weight by the model (i.e., be seen as more important) than any other feature.</p>\n",
    "<p>One way to correct for high variance is to use log normalization.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "dc": {
     "key": "59"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recency (months)      66.929\n",
       "Frequency (times)     33.830\n",
       "Time (months)        611.147\n",
       "monetary_log           0.837\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train_normed,X_test_normed = X_train.copy(), X_test.copy()\n",
    "\n",
    "col_to_normalize = 'Monetary (c.c. blood)'\n",
    "\n",
    "for df_ in [X_train_normed, X_test_normed]:\n",
    "    \n",
    "    df_['monetary_log'] = np.log(df_['Monetary (c.c. blood)'])\n",
    "    \n",
    "    df_.drop(columns='Monetary (c.c. blood)', inplace=True)\n",
    "\n",
    "\n",
    "X_train_normed.var().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "66"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 10. Training the logistic regression model\n",
    "<p>The variance looks much better now. Notice that now <code>Time (months)</code> has the largest variance, but it's not the <a href=\"https://en.wikipedia.org/wiki/Order_of_magnitude\">orders of magnitude</a> higher than the rest of the variables, so we'll leave it as is.</p>\n",
    "<p>We are now ready to train the logistic regression model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "dc": {
     "key": "66"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC score: 0.7891\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logreg = linear_model.LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "logreg.fit(X_train_normed, y_train)\n",
    "\n",
    "# AUC score for tpot model\n",
    "logreg_auc_score = roc_auc_score(y_test, logreg.predict_proba(X_test_normed)[:, 1])\n",
    "print(f'\\nAUC score: {logreg_auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "73"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 11. Conclusion\n",
    "<p>The demand for blood fluctuates throughout the year. As one <a href=\"https://www.kjrh.com/news/local-news/red-cross-in-blood-donation-crisis\">prominent</a> example, blood donations slow down during busy holiday seasons. An accurate forecast for the future supply of blood allows for an appropriate action to be taken ahead of time and therefore saving more lives.</p>\n",
    "<p>In this notebook, we explored automatic model selection using TPOT and AUC score we got was 0.7850. This is better than simply choosing <code>0</code> all the time (the target incidence suggests that such a model would have 76% success rate). We then log normalized our training data and improved the AUC score by 0.5%. In the field of machine learning, even small improvements in accuracy can be important, depending on the purpose.</p>\n",
    "<p>Another benefit of using logistic regression model is that it is interpretable. We can analyze how much of the variance in the response variable (<code>target</code>) can be explained by other variables in our dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "dc": {
     "key": "73"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logreg', 0.7890972663699937), ('tpot', 0.7637476160203432)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# Sort models based on their AUC score from highest to lowest\n",
    "sorted(\n",
    "    [('tpot', tpot_auc_score), ('logreg', logreg_auc_score)],\n",
    "    key=itemgetter(1),\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "73"
    }
   },
   "source": [
    "## References\n",
    "The math behind Logistic Regression, Rai K., 2020, Available at:https://medium.com/analytics-vidhya/the-math-behind-logistic-regression-c2f04ca27bca\n",
    "\n",
    "Hills, S., Eraso, Y. Factors associated with non-adherence to social distancing rules during the COVID-19 pandemic: a logistic regression analysis. BMC Public Health 21, 352 (2021).\n",
    "\n",
    "Red Cross in blood donation 'crisis', KJRH, Available at:https://www.kjrh.com/news/local-news/red-cross-in-blood-donation-crisis\n",
    "\n",
    "Blood transfusion - what to know if you get one, Webmd, Available at:https://www.webmd.com/a-to-z-guides/blood-transfusion-what-to-know#1\n",
    "\n",
    "Classification ROC and AUC, Google Developers, Available at:https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
